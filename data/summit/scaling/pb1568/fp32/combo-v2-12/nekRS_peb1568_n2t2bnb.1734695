                 __    ____  _____
   ____   ___   / /__ / __ \/ ___/
  / __ \ / _ \ / //_// /_/ /\__ \ 
 / / / //  __// ,<  / _, _/___/ / 
/_/ /_/ \___//_/|_|/_/ |_|/____/  v21.2.0 (89cacef2)

COPYRIGHT (c) 2019-2021 UCHICAGO ARGONNE, LLC

MPI tasks: 72

reading par file ...
general::filtering is deprecated and might be removed in the future!
general::filtermodes is deprecated and might be removed in the future!
general::filterweight is deprecated and might be removed in the future!

using NEKRS_HOME: /ccs/home/malachi/.local/nekrs
using NEKRS_CACHE_DIR: /gpfs/alpine/csc262/scratch/malachi/siam-pp-22/pb1568/combo-v2-fp32-12/.cache
using OCCA_CACHE_DIR: /gpfs/alpine/csc262/scratch/malachi/siam-pp-22/pb1568/combo-v2-fp32-12/.cache/occa/

Initializing device 
active occa mode: CUDA

building udf ... 
Consolidate compiler generated dependencies of target UDF
[100%] Built target UDF
done (0.707649s)
skip building nekInterface (SIZE requires no update)
loading nek ... 
done
loading udf kernels ... done (0.0454186s)

loading kernels ... done (4.10775s)

 Reading /gpfs/alpine/csc262/scratch/malachi/siam-pp-22/pb1568/combo-v2-fp32-12/peb1568_n2t2bnb.re2                                          
 reading mesh 
 reading bc for ifld           1
 done :: read .re2 file    0.80     sec

Running parCon ... (tol=0.2)

 Error: elementCheck
Running parCon ... (tol=0.02)
Running parRSB ...
Warning: Partitioner only reached a tolerance of 1.280332 given 0.000500 after 50 x 50 iterations in Level=1!
parRSB finished in 4.96398 s

 reading mesh 
 reading curved sides 
 reading bc for ifld           1
 done :: read .re2 file     1.3     sec

 setup mesh topology
   Right-handed check complete for      524386 elements. OK.
gs_setup: 4530143 unique labels shared
   handle bytes (avg, min, max): 2.55966e+07 24947436 26204412
   buffer bytes (avg, min, max): 2.06179e+06 1428432 2694016
   setupds time 8.3903E-01 seconds   0  8    70904907      524386
 
 nElements   max/min/bal: 7284 7283 1.00
 nMessages   max/min/avg: 20 7 12.19
 msgSize     max/min/avg: 42392 1 10817.14
 msgSizeSum  max/min/avg: 168376 89277 128862.00
 
 max multiplicity           44
 done :: setup mesh topology
  
 call usrdat
 done :: usrdat

 generate geometry data
 done :: generate geometry data
  
 call usrdat2
 done :: usrdat2

  3.9629E-15  3.9299E-15  7.1054E-15  7.6927E-16  7.6927E-16  9.8206E-16 xyz repair 1
  3.9629E-15  3.9299E-15  7.1054E-15  6.9253E-15  6.5223E-15  9.3390E-15 xyz repair 2
  3.7848E-15  3.5527E-15  7.1054E-15  5.8127E-15  6.1523E-15  8.9797E-15 xyz repair 3
  3.7863E-15  2.8779E-15  3.2674E-15  3.7863E-15  2.8779E-15  3.2674E-15 xyz repair 4
 regenerate geometry data           1
 done :: regenerate geometry data           1
  
 regenerate geometry data           1
 done :: regenerate geometry data           1
  
 verify mesh topology
  -13.858251128023300        13.858078221547562       Xrange
  -13.858092645782323        13.858190303122345       Yrange
  -14.673319816589355        17.691219329833984       Zrange
 done :: verify mesh topology
  
 mesh metrics:
 GLL grid spacing min/max    : 2.21E-04 3.02E-01
 scaled Jacobian  min/max/avg: 2.59E-02 9.91E-01 3.71E-01
 aspect ratio     min/max/avg: 1.12E+00 1.08E+02 1.26E+01

 call usrdat3
 done :: usrdat3

gridpoints unique/tot:     184172283    268485632
dofs vel/pr:               175531272    184064426
 nek setup done in    1.4621E+01 s

 set initial conditions
 Checking restart options: r5.fld                                                                                                                              
 nekuic (1) for ifld            1
 Reading checkpoint data 
       FILE:/gpfs/alpine/csc262/scratch/malachi/siam-pp-22/pb1568/combo-v2-fp32-12/r5.fld                                                       

        0  2.0000E+01 done :: Read checkpoint data
                              avg data-throughput =     2.1GB/s
                              io-nodes =    72

 xyz min    -13.858      -13.858      -14.673    
 uvwpt min  -6.4231      -7.8486      -7.9142      -20.893       0.0000    
 PS min      0.0000       0.0000       0.0000      0.99000E+22
 xyz max     13.858       13.858       17.691    
 uvwpt max   6.5953       7.1037       10.614       19.461       0.0000    
 PS max      0.0000       0.0000       0.0000     -0.99000E+22
 Restart: recompute geom. factors.
 regenerate geometry data           1
 done :: regenerate geometry data           1
  
 done :: set initial conditions
  
calling nek_userchk ...
 xyz min    -13.858      -13.858      -14.673    
 xyz max     13.858       13.858       17.691    

loading mesh from nek ... NboundaryIDs: 4, NboundaryFaces: 178458 done (0.000915709s)
generating mesh ... Nq: 8 cubNq: 11
computing geometric factors ... J [1.11222e-05,0.168148] done (0.474769s)
timing oogs modes: 0.000698069s 0.000398983s 0.000401278s 0.000420301s 0.000318248s 0.000282884s used config: 3.0.1
min 56% of the local elements are internal
timing oogs modes: 0.00243161s 0.000996468s 0.000981441s 0.00100658s 0.00077135s 0.000747665s used config: 3.0.1
loading ns kernels ... done (0.000520319s)
copying solution from nek
calling udf_setup ... done
copying solution to nek
================ ELLIPTIC SETUP VELOCITY ================
bID 1 -> bcType fixedValue
bID 2 -> bcType zeroGradient
bID 3 -> bcType zeroValue
bID 4 -> bcType zeroValue
allNeumann = 0 
loading elliptic kernels ... done (0.000177466s)
timing oogs modes: 0.0022911s 0.000997368s 0.00099859s 0.000974983s 0.000751512s 0.000760002s used config: 3.0.0
timing oogs modes: 0.00232682s 0.0016668s 0.00163606s 0.00163511s 0.000741155s 0.00107261s used config: 3.0.0
building Jacobi preconditioner ... done (0.316444s)
done (1.96542s)
================ ELLIPTIC SETUP PRESSURE ================
allNeumann = 0 
loading elliptic kernels ... done (0.000199454s)
timing oogs modes: 0.000684819s 0.000420903s 0.000417501s 0.000412087s 0.000310402s 0.000288395s used config: 3.0.1
timing oogs modes: 0.000720834s 0.000684955s 0.000689213s 0.000661183s 0.00045667s 0.000436162s used config: 3.0.1
building MG preconditioner ... 
loading elliptic preconditioner kernels ... done (0.000230776s)
=============BUILDING MULTIGRID LEVEL OF DEGREE 7==================
timing oogs modes: 0.000659161s 0.000275435s 0.000266595s 0.000268308s 0.000239782s 0.000219437s used config: 3.0.1
timing oogs modes: 0.000632624s 0.000403863s 0.000397839s 0.000398185s 0.000290225s 0.00026919s used config: 3.0.1
timing oogs modes: 0.000731877s 0.00032901s 0.000316681s 0.000309817s 0.00023114s 0.000217624s used config: 3.0.1
estimating maxEigenvalue ... 12.4543 done (0.579431s)
=============BUILDING MULTIGRID LEVEL OF DEGREE 5==================
computing geometric factors ... J [1.11206e-05,0.168147] done (0.187866s)
loading elliptic preconditioner kernels ... done (0.000269219s)
done (0.0171399s)
timing oogs modes: 0.000375088s 0.000207166s 0.000210812s 0.000203251s 0.000225243s 0.000203129s used config: 3.0.1
timing oogs modes: 0.000407503s 0.000284045s 0.000282832s 0.000285533s 0.000255777s 0.000225566s used config: 3.0.1
setup SEMFEM preconditioner ... 
building matrix ... done (20.5618s)
AMGX version 2.2.0.132-opensource
Built on Jan  2 2022, 11:45:05
Compiled with CUDA Runtime 11.0, using CUDA driver 11.0
Using CUDA-Aware MPI (GPU Direct) communicator...
AMG Grid:
         Number of Levels: 11
            LVL         ROWS               NNZ    SPRSTY       Mem (GB)
         --------------------------------------------------------------
           0(D)     67678308        1264310176  2.76e-07           11.7
           1(D)     28799257         953408965  1.15e-06           17.7
           2(D)     11178460         543029976  4.35e-06           11.2
           3(D)      3592950         263963344  2.04e-05           6.36
           4(D)       861811          88269543  0.000119           2.74
           5(D)       113498          11780688  0.000915          0.579
           6(D)        12807           1279589    0.0078          0.131
           7(D)         1583            132901     0.053         0.0357
           8(D)          157              8877      0.36        0.00493
           9(D)           17               259     0.896       6.25e-05
          10(D)            3                 9         1       2.06e-06
         --------------------------------------------------------------
         Grid Complexity: 1.65842
         Operator Complexity: 2.47264
         Total Memory Usage: 50.5323 GB
         --------------------------------------------------------------
done (50.82s)
--------------------Multigrid Report---------------------
---------------------------------------------------------
level|    Type    |                 |     Smoother      |
     |            |                 |                   |
---------------------------------------------------------
   0 |    pMG     |   Matrix-free   | Chebyshev+Schwarz |
     |            |     Degree  7   |                   |
   1 |    AMG     |   Matrix        | AMGX              |
     |            |     Degree  5   |                   |
---------------------------------------------------------
done (70.2527s)
done (74.4139s)

settings:

key: ADVECTION,                                               value: TRUE
key: ADVECTION TYPE,                                          value: CUBATURE+CONVECTIVE
key: AMG SOLVER,                                              value: AMGX
key: AMG SOLVER LOCATION,                                     value: GPU
key: AMG SOLVER PRECISION,                                    value: FP32
key: AMGX CONFIG FILE,                                        value: amgx.json
key: BUILD ONLY,                                              value: FALSE
key: CASENAME,                                                value: peb1568_n2t2bnb
key: CHECKPOINT OUTPUT MESH,                                  value: FALSE
key: CI-MODE,                                                 value: 0
key: CONSTANT FLOW RATE,                                      value: FALSE
key: CUBATURE POLYNOMIAL DEGREE,                              value: 10
key: DATA FILE,                                               value: /gpfs/alpine/csc262/scratch/malachi/siam-pp-22/pb1568/combo-v2-fp32-12/.cache/udf/udf.okl
key: DENSITY,                                                 value: 1.000000e+00
key: DEVICE NUMBER,                                           value: 0
key: DT,                                                      value: 5.000000e-04
key: ELEMENT MAP,                                             value: ISOPARAMETRIC
key: ELEMENT TYPE,                                            value: 12
key: ELLIPTIC INTEGRATION,                                    value: NODAL
key: ENABLE FLOATCOMMHALF GS SUPPORT,                         value: FALSE
key: ENABLE OVERLAP,                                          value: TRUE
key: FORMAT,                                                  value: 1.0
key: GALERKIN COARSE MATRIX,                                  value: FALSE
key: HPFRT MODES,                                             value: 2.000000e+00
key: HPFRT STRENGTH,                                          value: 2.000000e+02
key: MESH DIMENSION,                                          value: 3
key: MESH FILE,                                               value: peb1568_n2t2bnb.re2
key: MESH INTEGRATION ORDER,                                  value: 3
key: MOVING MESH,                                             value: FALSE
key: NEK USR FILE,                                            value: peb1568_n2t2bnb.usr
key: NUMBER OF SCALARS,                                       value: 0
key: NUMBER TIMESTEPS,                                        value: 2000
key: PARALMOND SMOOTH COARSEST,                               value: FALSE
key: PLATFORM NUMBER,                                         value: 0
key: POLYNOMIAL DEGREE,                                       value: 7
key: PRESSURE BASIS,                                          value: NODAL
key: PRESSURE DISCRETIZATION,                                 value: CONTINUOUS
key: PRESSURE INITIAL GUESS,                                  value: PROJECTION-ACONJ
key: PRESSURE KRYLOV SOLVER,                                  value: PGMRES+FLEXIBLE
key: PRESSURE MAXIMUM ITERATIONS,                             value: 200
key: PRESSURE MULTIGRID CHEBYSHEV DEGREE,                     value: 2
key: PRESSURE MULTIGRID CHEBYSHEV MAX EIGENVALUE BOUND FACTOR,value: 1.1
key: PRESSURE MULTIGRID CHEBYSHEV MIN EIGENVALUE BOUND FACTOR,value: 0.1
key: PRESSURE MULTIGRID COARSE SEMFEM,                        value: TRUE
key: PRESSURE MULTIGRID COARSE SOLVE,                         value: TRUE
key: PRESSURE MULTIGRID COARSENING,                           value: 7,5
key: PRESSURE MULTIGRID DOWNWARD SMOOTHER,                    value: ASM
key: PRESSURE MULTIGRID SMOOTHER,                             value: CHEBYSHEV+ASM
key: PRESSURE MULTIGRID UPWARD SMOOTHER,                      value: ASM
key: PRESSURE PARALMOND CYCLE,                                value: VCYCLE
key: PRESSURE PRECONDITIONER,                                 value: MULTIGRID
key: PRESSURE RESIDUAL PROJECTION START,                      value: 5
key: PRESSURE RESIDUAL PROJECTION VECTORS,                    value: 10
key: PRESSURE SEMFEM SOLVER,                                  value: AMGX
key: PRESSURE SEMFEM SOLVER PRECISION,                        value: FP32
key: PRESSURE SOLVER TOLERANCE,                               value: 1.000000e-04
key: REGULARIZATION METHOD,                                   value: RELAXATION
key: RESTART FILE NAME,                                       value: r5.fld
key: RESTART FROM FILE,                                       value: 1
key: SCALAR MAXIMUM ITERATIONS,                               value: 200
key: SOLUTION OUTPUT CONTROL,                                 value: STEPS
key: SOLUTION OUTPUT INTERVAL,                                value: 0.000000
key: START TIME,                                              value: 2.000000e+01
key: STRESSFORMULATION,                                       value: FALSE
key: SUBCYCLING STEPS,                                        value: 2
key: SUBCYCLING TIME ORDER,                                   value: 4
key: SUBCYCLING TIME STAGE NUMBER,                            value: 4
key: THREAD MODEL,                                            value: CUDA
key: TIME INTEGRATOR,                                         value: TOMBO2
key: UDF FILE,                                                value: peb1568_n2t2bnb.udf
key: UDF OKL FILE,                                            value: peb1568_n2t2bnb.oudf
key: VARIABLE DT,                                             value: FALSE
key: VELOCITY BASIS,                                          value: NODAL
key: VELOCITY BLOCK SOLVER,                                   value: TRUE
key: VELOCITY COEFF FIELD,                                    value: TRUE
key: VELOCITY DISCRETIZATION,                                 value: CONTINUOUS
key: VELOCITY HPFRT MODES,                                    value: 2.000000e+00
key: VELOCITY HPFRT STRENGTH,                                 value: 2.000000e+02
key: VELOCITY KRYLOV SOLVER,                                  value: PCG
key: VELOCITY MAXIMUM ITERATIONS,                             value: 200
key: VELOCITY PRECONDITIONER,                                 value: JACOBI
key: VELOCITY REGULARIZATION METHOD,                          value: RELAXATION
key: VELOCITY SOLVER TOLERANCE,                               value: 1.000000e-06
key: VERBOSE,                                                 value: FALSE
key: VISCOSITY,                                               value: 1.000000e-04

occa memory usage: 6.50186 GB
initialization took 112.34 s

timestepping for 2000 steps ...
  projP  : resNorm0 1.21e+00  resNorm 1.21e+00  ratio = 1.000e+00  0/10
  P      : iter 030  resNorm0 1.21e+00  resNorm 6.23e-05
  UVW    : iter 003  resNorm0 1.17e-01  resNorm 4.27e-07  divErrNorms 9.76e-07 4.95e-01
step= 1  t= 2.00005000e+01  dt=5.0e-04  C= 2.05  UVW: 3  P: 30  eTimeStep= 5.75e+00s eTime= 5.74661e+00s
  projP  : resNorm0 1.50e+00  resNorm 1.50e+00  ratio = 1.000e+00  0/10
  P      : iter 036  resNorm0 1.50e+00  resNorm 9.26e-05
  UVW    : iter 003  resNorm0 1.76e-01  resNorm 2.10e-07  divErrNorms 9.27e-07 4.93e-01
step= 2  t= 2.00010000e+01  dt=5.0e-04  C= 1.98  UVW: 3  P: 36  eTimeStep= 2.67e+00s eTime= 8.41662e+00s
  projP  : resNorm0 3.99e-01  resNorm 3.99e-01  ratio = 1.000e+00  0/10
  P      : iter 024  resNorm0 3.99e-01  resNorm 7.66e-05
  UVW    : iter 003  resNorm0 1.75e-01  resNorm 2.09e-07  divErrNorms 8.69e-07 4.90e-01
step= 3  t= 2.00015000e+01  dt=5.0e-04  C= 1.93  UVW: 3  P: 24  eTimeStep= 1.64e+00s eTime= 1.00597e+01s
  projP  : resNorm0 2.17e-01  resNorm 2.17e-01  ratio = 1.000e+00  0/10
  P      : iter 200  resNorm0 2.17e-01  resNorm 1.60e-02
  UVW    : iter 003  resNorm0 1.75e-01  resNorm 2.08e-07  divErrNorms 1.10e-06 4.87e-01
step= 4  t= 2.00020000e+01  dt=5.0e-04  C= 1.96  UVW: 3  P: 200  eTimeStep= 1.43e+01s eTime= 2.43258e+01s
  projP  : resNorm0 1.74e-01  resNorm 1.74e-01  ratio = 1.000e+00  0/10
  P      : iter 200  resNorm0 1.74e-01  resNorm 1.74e-01
  UVW    : iter 003  resNorm0 1.75e-01  resNorm 2.07e-07  divErrNorms 1.43e-06 4.85e-01
step= 5  t= 2.00025000e+01  dt=5.0e-04  C= 1.98  UVW: 3  P: 200  eTimeStep= 1.47e+01s eTime= 3.89964e+01s
Unreasonable res0Norm!
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 18 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 42 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 2 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 19 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 43 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 6 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 44 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 7 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 12 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 21 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 45 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 5 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 8 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 46 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 22 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 47 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 66 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 9 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 54 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 1 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 20 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 67 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 10 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 55 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 13 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 23 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 4 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 68 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 11 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 56 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 69 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 49 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 57 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 70 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 59 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 71 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 58 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 48 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 3 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 30 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 24 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 14 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 31 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 32 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 33 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 50 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 34 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 25 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 16 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 35 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 51 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 26 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 15 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 52 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 27 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 53 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 28 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 29 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 17 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 36 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 60 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 37 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 61 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 38 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 62 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 40 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 63 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 39 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 64 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 41 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 65 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

------------------------------------------------------------
Sender: LSF System <lsfadmin@batch5>
Subject: Job 1734695: <nekRS_peb1568_n2t2bnb> in cluster <summit> Exited

Job <nekRS_peb1568_n2t2bnb> was submitted from host <login3> by user <malachi> in cluster <summit> at Mon Jan  3 10:19:52 2022
Job was executed on host(s) <1*batch5>, in queue <batch>, as user <malachi> in cluster <summit> at Mon Jan  3 16:06:29 2022
                            <42*g22n13>
                            <42*g23n08>
                            <42*g27n17>
                            <42*g27n18>
                            <42*g28n01>
                            <42*g28n05>
                            <42*g28n06>
                            <42*g30n06>
                            <42*g30n08>
                            <42*g30n09>
                            <42*g30n10>
                            <42*g30n11>
</ccs/home/malachi> was used as the home directory.
</gpfs/alpine/scratch/malachi/csc262/siam-pp-22/pb1568/combo-v2-fp32-12> was used as the working directory.
Started at Mon Jan  3 16:06:29 2022
Terminated at Mon Jan  3 16:09:44 2022
Results reported at Mon Jan  3 16:09:44 2022

The output (if any) is above this job summary.

