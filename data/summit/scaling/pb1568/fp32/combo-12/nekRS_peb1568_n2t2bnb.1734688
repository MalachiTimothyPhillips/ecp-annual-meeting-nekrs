                 __    ____  _____
   ____   ___   / /__ / __ \/ ___/
  / __ \ / _ \ / //_// /_/ /\__ \ 
 / / / //  __// ,<  / _, _/___/ / 
/_/ /_/ \___//_/|_|/_/ |_|/____/  v21.2.0 (89cacef2)

COPYRIGHT (c) 2019-2021 UCHICAGO ARGONNE, LLC

MPI tasks: 72

reading par file ...
general::filtering is deprecated and might be removed in the future!
general::filtermodes is deprecated and might be removed in the future!
general::filterweight is deprecated and might be removed in the future!

using NEKRS_HOME: /ccs/home/malachi/.local/nekrs
using NEKRS_CACHE_DIR: /gpfs/alpine/csc262/scratch/malachi/siam-pp-22/pb1568/combo-fp32-12/.cache
using OCCA_CACHE_DIR: /gpfs/alpine/csc262/scratch/malachi/siam-pp-22/pb1568/combo-fp32-12/.cache/occa/

Initializing device 
active occa mode: CUDA

building udf ... 
Consolidate compiler generated dependencies of target UDF
[100%] Built target UDF
done (0.954039s)
skip building nekInterface (SIZE requires no update)
loading nek ... 
done
loading udf kernels ... done (0.0915097s)

loading kernels ... done (3.16633s)

 Reading /gpfs/alpine/csc262/scratch/malachi/siam-pp-22/pb1568/combo-fp32-12/peb1568_n2t2bnb.re2                                             
 reading mesh 
 reading bc for ifld           1
 done :: read .re2 file     1.1     sec

Running parCon ... (tol=0.2)

 Error: elementCheck
Running parCon ... (tol=0.02)
Running parRSB ...
Warning: Partitioner only reached a tolerance of 1.280332 given 0.000500 after 50 x 50 iterations in Level=1!
parRSB finished in 5.01001 s

 reading mesh 
 reading curved sides 
 reading bc for ifld           1
 done :: read .re2 file     1.3     sec

 setup mesh topology
   Right-handed check complete for      524386 elements. OK.
gs_setup: 4530143 unique labels shared
   handle bytes (avg, min, max): 2.55966e+07 24947436 26204412
   buffer bytes (avg, min, max): 2.06179e+06 1428432 2694016
   setupds time 8.3585E-01 seconds   0  8    70904907      524386
 
 nElements   max/min/bal: 7284 7283 1.00
 nMessages   max/min/avg: 20 7 12.19
 msgSize     max/min/avg: 42392 1 10817.14
 msgSizeSum  max/min/avg: 168376 89277 128862.00
 
 max multiplicity           44
 done :: setup mesh topology
  
 call usrdat
 done :: usrdat

 generate geometry data
 done :: generate geometry data
  
 call usrdat2
 done :: usrdat2

  3.9629E-15  3.9299E-15  7.1054E-15  7.6927E-16  7.6927E-16  9.8206E-16 xyz repair 1
  3.9629E-15  3.9299E-15  7.1054E-15  6.9253E-15  6.5223E-15  9.3390E-15 xyz repair 2
  3.7848E-15  3.5527E-15  7.1054E-15  5.8127E-15  6.1523E-15  8.9797E-15 xyz repair 3
  3.7863E-15  2.8779E-15  3.2674E-15  3.7863E-15  2.8779E-15  3.2674E-15 xyz repair 4
 regenerate geometry data           1
 done :: regenerate geometry data           1
  
 regenerate geometry data           1
 done :: regenerate geometry data           1
  
 verify mesh topology
  -13.858251128023300        13.858078221547562       Xrange
  -13.858092645782323        13.858190303122345       Yrange
  -14.673319816589355        17.691219329833984       Zrange
 done :: verify mesh topology
  
 mesh metrics:
 GLL grid spacing min/max    : 2.21E-04 3.02E-01
 scaled Jacobian  min/max/avg: 2.59E-02 9.91E-01 3.71E-01
 aspect ratio     min/max/avg: 1.12E+00 1.08E+02 1.26E+01

 call usrdat3
 done :: usrdat3

gridpoints unique/tot:     184172283    268485632
dofs vel/pr:               175531272    184064426
 nek setup done in    1.4948E+01 s

 set initial conditions
 Checking restart options: r5.fld                                                                                                                              
 nekuic (1) for ifld            1
 Reading checkpoint data 
       FILE:/gpfs/alpine/csc262/scratch/malachi/siam-pp-22/pb1568/combo-fp32-12/r5.fld                                                          

        0  2.0000E+01 done :: Read checkpoint data
                              avg data-throughput =     2.1GB/s
                              io-nodes =    72

 xyz min    -13.858      -13.858      -14.673    
 uvwpt min  -6.4231      -7.8486      -7.9142      -20.893       0.0000    
 PS min      0.0000       0.0000       0.0000      0.99000E+22
 xyz max     13.858       13.858       17.691    
 uvwpt max   6.5953       7.1037       10.614       19.461       0.0000    
 PS max      0.0000       0.0000       0.0000     -0.99000E+22
 Restart: recompute geom. factors.
 regenerate geometry data           1
 done :: regenerate geometry data           1
  
 done :: set initial conditions
  
calling nek_userchk ...
 xyz min    -13.858      -13.858      -14.673    
 xyz max     13.858       13.858       17.691    

loading mesh from nek ... NboundaryIDs: 4, NboundaryFaces: 178458 done (0.00091245s)
generating mesh ... Nq: 8 cubNq: 11
computing geometric factors ... J [1.11222e-05,0.168148] done (0.481598s)
timing oogs modes: 0.000703976s 0.000412998s 0.000420638s 0.000405486s 0.000318283s 0.000285008s used config: 3.0.1
min 56% of the local elements are internal
timing oogs modes: 0.00232376s 0.000990346s 0.00101261s 0.00097847s 0.000742642s 0.000763253s used config: 3.0.0
loading ns kernels ... done (0.000570526s)
copying solution from nek
calling udf_setup ... done
copying solution to nek
================ ELLIPTIC SETUP VELOCITY ================
bID 1 -> bcType fixedValue
bID 2 -> bcType zeroGradient
bID 3 -> bcType zeroValue
bID 4 -> bcType zeroValue
allNeumann = 0 
loading elliptic kernels ... done (0.000176201s)
timing oogs modes: 0.002413s 0.000983069s 0.000990449s 0.000991708s 0.000744778s 0.0007008s used config: 3.0.1
timing oogs modes: 0.00228643s 0.00163738s 0.00164607s 0.00163895s 0.000769511s 0.000728076s used config: 3.0.1
building Jacobi preconditioner ... done (0.318484s)
done (1.88915s)
================ ELLIPTIC SETUP PRESSURE ================
allNeumann = 0 
loading elliptic kernels ... done (0.00019501s)
timing oogs modes: 0.00068429s 0.000404181s 0.000405549s 0.00041051s 0.00033381s 0.00027666s used config: 3.0.1
timing oogs modes: 0.000727905s 0.000698358s 0.000679946s 0.000659335s 0.00045649s 0.000438635s used config: 3.0.1
building MG preconditioner ... 
loading elliptic preconditioner kernels ... done (0.00022697s)
=============BUILDING MULTIGRID LEVEL OF DEGREE 7==================
timing oogs modes: 0.00060142s 0.000277969s 0.000263128s 0.000273516s 0.000246087s 0.000222607s used config: 3.0.1
timing oogs modes: 0.000629236s 0.000401985s 0.000397658s 0.000391228s 0.000308463s 0.000262526s used config: 3.0.1
timing oogs modes: 0.000682091s 0.000309726s 0.000320354s 0.000317063s 0.000243963s 0.000213273s used config: 3.0.1
estimating maxEigenvalue ... 12.4543 done (0.560503s)
=============BUILDING MULTIGRID LEVEL OF DEGREE 6==================
computing geometric factors ... J [1.11206e-05,0.168146] done (0.25625s)
loading elliptic preconditioner kernels ... done (0.000310406s)
done (0.0214772s)
timing oogs modes: 0.000485698s 0.000235948s 0.000242174s 0.000235766s 0.000229009s 0.00021103s used config: 3.0.1
timing oogs modes: 0.000515055s 0.000349195s 0.000345841s 0.000341049s 0.000264375s 0.000244742s used config: 3.0.1
setup SEMFEM preconditioner ... 
building matrix ... done (35.753s)
AMGX version 2.2.0.132-opensource
Built on Jan  2 2022, 11:45:05
Compiled with CUDA Runtime 11.0, using CUDA driver 11.0
Using CUDA-Aware MPI (GPU Direct) communicator...
AMG Grid:
         Number of Levels: 11
            LVL         ROWS               NNZ    SPRSTY       Mem (GB)
         --------------------------------------------------------------
           0(D)    116345378        2179320876  1.61e-07           19.9
           1(D)     49352374        1646460474  6.76e-07             30
           2(D)     18750255         935233051  2.66e-06           18.7
           3(D)      6167243         460348993  1.21e-05           10.5
           4(D)      1506398         149734404   6.6e-05           4.23
           5(D)       210709          22633779   0.00051          0.955
           6(D)        22746           2147126   0.00415          0.173
           7(D)         2572            219602    0.0332         0.0462
           8(D)          273             18679     0.251        0.00991
           9(D)           36              1102      0.85       0.000453
          10(D)            4                16         1       3.04e-06
         --------------------------------------------------------------
         Grid Complexity: 1.65334
         Operator Complexity: 2.47605
         Total Memory Usage: 84.5067 GB
         --------------------------------------------------------------
done (72.7145s)
--------------------Multigrid Report---------------------
---------------------------------------------------------
level|    Type    |                 |     Smoother      |
     |            |                 |                   |
---------------------------------------------------------
   0 |    pMG     |   Matrix-free   | Chebyshev+Schwarz |
     |            |     Degree  7   |                   |
   1 |    AMG     |   Matrix        | AMGX              |
     |            |     Degree  6   |                   |
---------------------------------------------------------
done (93.8777s)
done (98.1108s)

settings:

key: ADVECTION,                                               value: TRUE
key: ADVECTION TYPE,                                          value: CUBATURE+CONVECTIVE
key: AMG SOLVER,                                              value: AMGX
key: AMG SOLVER LOCATION,                                     value: GPU
key: AMG SOLVER PRECISION,                                    value: FP32
key: AMGX CONFIG FILE,                                        value: amgx.json
key: BUILD ONLY,                                              value: FALSE
key: CASENAME,                                                value: peb1568_n2t2bnb
key: CHECKPOINT OUTPUT MESH,                                  value: FALSE
key: CI-MODE,                                                 value: 0
key: CONSTANT FLOW RATE,                                      value: FALSE
key: CUBATURE POLYNOMIAL DEGREE,                              value: 10
key: DATA FILE,                                               value: /gpfs/alpine/csc262/scratch/malachi/siam-pp-22/pb1568/combo-fp32-12/.cache/udf/udf.okl
key: DENSITY,                                                 value: 1.000000e+00
key: DEVICE NUMBER,                                           value: 0
key: DT,                                                      value: 5.000000e-04
key: ELEMENT MAP,                                             value: ISOPARAMETRIC
key: ELEMENT TYPE,                                            value: 12
key: ELLIPTIC INTEGRATION,                                    value: NODAL
key: ENABLE FLOATCOMMHALF GS SUPPORT,                         value: FALSE
key: ENABLE OVERLAP,                                          value: TRUE
key: FORMAT,                                                  value: 1.0
key: GALERKIN COARSE MATRIX,                                  value: FALSE
key: HPFRT MODES,                                             value: 2.000000e+00
key: HPFRT STRENGTH,                                          value: 2.000000e+02
key: MESH DIMENSION,                                          value: 3
key: MESH FILE,                                               value: peb1568_n2t2bnb.re2
key: MESH INTEGRATION ORDER,                                  value: 3
key: MOVING MESH,                                             value: FALSE
key: NEK USR FILE,                                            value: peb1568_n2t2bnb.usr
key: NUMBER OF SCALARS,                                       value: 0
key: NUMBER TIMESTEPS,                                        value: 2000
key: PARALMOND SMOOTH COARSEST,                               value: FALSE
key: PLATFORM NUMBER,                                         value: 0
key: POLYNOMIAL DEGREE,                                       value: 7
key: PRESSURE BASIS,                                          value: NODAL
key: PRESSURE DISCRETIZATION,                                 value: CONTINUOUS
key: PRESSURE INITIAL GUESS,                                  value: PROJECTION-ACONJ
key: PRESSURE KRYLOV SOLVER,                                  value: PGMRES+FLEXIBLE
key: PRESSURE MAXIMUM ITERATIONS,                             value: 200
key: PRESSURE MULTIGRID CHEBYSHEV DEGREE,                     value: 2
key: PRESSURE MULTIGRID CHEBYSHEV MAX EIGENVALUE BOUND FACTOR,value: 1.1
key: PRESSURE MULTIGRID CHEBYSHEV MIN EIGENVALUE BOUND FACTOR,value: 0.1
key: PRESSURE MULTIGRID COARSE SEMFEM,                        value: TRUE
key: PRESSURE MULTIGRID COARSE SOLVE,                         value: TRUE
key: PRESSURE MULTIGRID COARSENING,                           value: 7,6
key: PRESSURE MULTIGRID DOWNWARD SMOOTHER,                    value: ASM
key: PRESSURE MULTIGRID SMOOTHER,                             value: CHEBYSHEV+ASM
key: PRESSURE MULTIGRID UPWARD SMOOTHER,                      value: ASM
key: PRESSURE PARALMOND CYCLE,                                value: VCYCLE
key: PRESSURE PRECONDITIONER,                                 value: MULTIGRID
key: PRESSURE RESIDUAL PROJECTION START,                      value: 5
key: PRESSURE RESIDUAL PROJECTION VECTORS,                    value: 10
key: PRESSURE SEMFEM SOLVER,                                  value: AMGX
key: PRESSURE SEMFEM SOLVER PRECISION,                        value: FP32
key: PRESSURE SOLVER TOLERANCE,                               value: 1.000000e-04
key: REGULARIZATION METHOD,                                   value: RELAXATION
key: RESTART FILE NAME,                                       value: r5.fld
key: RESTART FROM FILE,                                       value: 1
key: SCALAR MAXIMUM ITERATIONS,                               value: 200
key: SOLUTION OUTPUT CONTROL,                                 value: STEPS
key: SOLUTION OUTPUT INTERVAL,                                value: 0.000000
key: START TIME,                                              value: 2.000000e+01
key: STRESSFORMULATION,                                       value: FALSE
key: SUBCYCLING STEPS,                                        value: 2
key: SUBCYCLING TIME ORDER,                                   value: 4
key: SUBCYCLING TIME STAGE NUMBER,                            value: 4
key: THREAD MODEL,                                            value: CUDA
key: TIME INTEGRATOR,                                         value: TOMBO2
key: UDF FILE,                                                value: peb1568_n2t2bnb.udf
key: UDF OKL FILE,                                            value: peb1568_n2t2bnb.oudf
key: VARIABLE DT,                                             value: FALSE
key: VELOCITY BASIS,                                          value: NODAL
key: VELOCITY BLOCK SOLVER,                                   value: TRUE
key: VELOCITY COEFF FIELD,                                    value: TRUE
key: VELOCITY DISCRETIZATION,                                 value: CONTINUOUS
key: VELOCITY HPFRT MODES,                                    value: 2.000000e+00
key: VELOCITY HPFRT STRENGTH,                                 value: 2.000000e+02
key: VELOCITY KRYLOV SOLVER,                                  value: PCG
key: VELOCITY MAXIMUM ITERATIONS,                             value: 200
key: VELOCITY PRECONDITIONER,                                 value: JACOBI
key: VELOCITY REGULARIZATION METHOD,                          value: RELAXATION
key: VELOCITY SOLVER TOLERANCE,                               value: 1.000000e-06
key: VERBOSE,                                                 value: FALSE
key: VISCOSITY,                                               value: 1.000000e-04

occa memory usage: 6.69733 GB
initialization took 135.209 s

timestepping for 2000 steps ...
  projP  : resNorm0 1.21e+00  resNorm 1.21e+00  ratio = 1.000e+00  0/10
  P      : iter 024  resNorm0 1.21e+00  resNorm 9.70e-05
  UVW    : iter 003  resNorm0 1.17e-01  resNorm 4.27e-07  divErrNorms 9.75e-07 4.95e-01
step= 1  t= 2.00005000e+01  dt=5.0e-04  C= 2.05  UVW: 3  P: 24  eTimeStep= 5.99e+00s eTime= 5.98995e+00s
  projP  : resNorm0 1.50e+00  resNorm 1.50e+00  ratio = 1.000e+00  0/10
  P      : iter 022  resNorm0 1.50e+00  resNorm 8.99e-05
  UVW    : iter 003  resNorm0 1.76e-01  resNorm 2.10e-07  divErrNorms 9.18e-07 4.93e-01
step= 2  t= 2.00010000e+01  dt=5.0e-04  C= 1.98  UVW: 3  P: 22  eTimeStep= 1.68e+00s eTime= 7.66901e+00s
  projP  : resNorm0 3.99e-01  resNorm 3.99e-01  ratio = 1.000e+00  0/10
  P      : iter 017  resNorm0 3.99e-01  resNorm 9.52e-05
  UVW    : iter 003  resNorm0 1.75e-01  resNorm 2.09e-07  divErrNorms 8.67e-07 4.90e-01
step= 3  t= 2.00015000e+01  dt=5.0e-04  C= 1.93  UVW: 3  P: 17  eTimeStep= 1.28e+00s eTime= 8.94675e+00s
  projP  : resNorm0 2.17e-01  resNorm 2.17e-01  ratio = 1.000e+00  0/10
  P      : iter 012  resNorm0 2.17e-01  resNorm 7.45e-05
  UVW    : iter 003  resNorm0 1.75e-01  resNorm 2.08e-07  divErrNorms 8.23e-07 4.87e-01
step= 4  t= 2.00020000e+01  dt=5.0e-04  C= 1.97  UVW: 3  P: 12  eTimeStep= 8.84e-01s eTime= 9.83080e+00s
  projP  : resNorm0 1.71e-01  resNorm 1.71e-01  ratio = 1.000e+00  0/10
  P      : iter 011  resNorm0 1.71e-01  resNorm 9.92e-05
  UVW    : iter 003  resNorm0 1.75e-01  resNorm 2.07e-07  divErrNorms 7.91e-07 4.85e-01
step= 5  t= 2.00025000e+01  dt=5.0e-04  C= 1.98  UVW: 3  P: 11  eTimeStep= 8.06e-01s eTime= 1.06368e+01s
  projP  : resNorm0 1.54e-01  resNorm 3.45e-02  ratio = 4.468e+00  1/10
  P      : iter 010  resNorm0 3.45e-02  resNorm 6.50e-05
  UVW    : iter 003  resNorm0 1.75e-01  resNorm 2.07e-07  divErrNorms 7.60e-07 4.83e-01
step= 6  t= 2.00030000e+01  dt=5.0e-04  C= 1.98  UVW: 3  P: 10  eTimeStep= 7.69e-01s eTime= 1.14058e+01s
  projP  : resNorm0 1.47e-01  resNorm 1.43e-02  ratio = 1.023e+01  2/10
  P      : iter 200  resNorm0 1.43e-02  resNorm 2.56e-04
  UVW    : iter 003  resNorm0 1.75e-01  resNorm 2.06e-07  divErrNorms 7.34e-07 4.82e-01
step= 7  t= 2.00035000e+01  dt=5.0e-04  C= 1.97  UVW: 3  P: 200  eTimeStep= 1.42e+01s eTime= 2.56096e+01s
  projP  : resNorm0 1.43e-01  resNorm 6.79e-03  ratio = 2.107e+01  3/10
  P      : iter 200  resNorm0 6.79e-03  resNorm 6.78e-03
  UVW    : iter 003  resNorm0 1.75e-01  resNorm 2.06e-07  divErrNorms 7.16e-07 4.81e-01
step= 8  t= 2.00040000e+01  dt=5.0e-04  C= 1.94  UVW: 3  P: 200  eTimeStep= 1.45e+01s eTime= 4.01499e+01s
Unreasonable res0Norm!
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 48 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 49 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 50 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 51 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 53 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 18 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 52 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 19 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 3 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 42 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 20 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 2 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 44 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 23 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 43 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 22 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 45 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 1 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 54 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 46 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 21 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 6 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 30 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 47 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 5 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 66 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 55 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 67 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 7 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 4 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 31 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 68 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 69 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 56 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 70 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 24 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 8 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 34 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 71 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 26 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 57 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 9 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 12 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 33 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 27 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 58 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 25 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 10 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 35 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 13 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 29 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 59 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 11 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 32 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 28 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 60 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 14 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 61 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 15 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 36 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 63 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 17 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 37 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 62 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 16 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 38 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 64 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 39 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 65 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 40 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 41 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

------------------------------------------------------------
Sender: LSF System <lsfadmin@batch2>
Subject: Job 1734688: <nekRS_peb1568_n2t2bnb> in cluster <summit> Exited

Job <nekRS_peb1568_n2t2bnb> was submitted from host <login3> by user <malachi> in cluster <summit> at Mon Jan  3 10:15:57 2022
Job was executed on host(s) <1*batch2>, in queue <batch>, as user <malachi> in cluster <summit> at Mon Jan  3 13:23:40 2022
                            <42*g24n03>
                            <42*g24n04>
                            <42*g31n16>
                            <42*h34n17>
                            <42*h35n09>
                            <42*h35n10>
                            <42*h35n11>
                            <42*h35n12>
                            <42*h35n14>
                            <42*h35n15>
                            <42*h35n16>
                            <42*h35n18>
</ccs/home/malachi> was used as the home directory.
</gpfs/alpine/scratch/malachi/csc262/siam-pp-22/pb1568/combo-fp32-12> was used as the working directory.
Started at Mon Jan  3 13:23:40 2022
Terminated at Mon Jan  3 13:27:20 2022
Results reported at Mon Jan  3 13:27:20 2022

The output (if any) is above this job summary.

