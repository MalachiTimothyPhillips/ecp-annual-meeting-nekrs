                 __    ____  _____
   ____   ___   / /__ / __ \/ ___/
  / __ \ / _ \ / //_// /_/ /\__ \ 
 / / / //  __// ,<  / _, _/___/ / 
/_/ /_/ \___//_/|_|/_/ |_|/____/  v21.1 (63a5a122)

COPYRIGHT (c) 2019-2021 UCHICAGO ARGONNE, LLC

MPI tasks: 132

reading par file ...
general::filtering is deprecated and might be removed in the future!
general::filtermodes is deprecated and might be removed in the future!
general::filterweight is deprecated and might be removed in the future!

using NEKRS_HOME: /ccs/home/malachi/.local/nekrs-next
using NEKRS_CACHE_DIR: /gpfs/alpine/csc262/scratch/malachi/siam-pp-22/pb1568/semfem-fp32-22/.cache
using OCCA_CACHE_DIR: /gpfs/alpine/csc262/scratch/malachi/siam-pp-22/pb1568/semfem-fp32-22/.cache/occa/

Initializing device 
active occa mode: CUDA

building udf ... 
[100%] Built target UDF
done (1.44812s)
loading nek ... 
done
loading udf kernels ... done (0.134477s)

loading kernels ... done (68.3379s)

 Reading /gpfs/alpine/csc262/scratch/malachi/siam-pp-22/pb1568/semfem-fp32-22/peb1568_n2t2bnb.re2                                            
 reading mesh 
 reading bc for ifld           1
 done :: read .re2 file    0.84     sec

Running parCon ... (tol=0.2)

 Error: elementCheck
Running parCon ... (tol=0.02)
Running parRSB ...
Warning: Partitioner only reached a tolerance of 1.280447 given 0.000500 after 50 x 50 iterations in Level=1!
parRSB finished in 3.47884 s

 reading mesh 
 reading curved sides 
 reading bc for ifld           1
 done :: read .re2 file    0.57     sec

 setup mesh topology
   Right-handed check complete for      524386 elements. OK.
gs_setup: 5672495 unique labels shared
   handle bytes (avg, min, max): 1.42279e+07 13843580 14693580
   buffer bytes (avg, min, max): 1.41805e+06 1014528 1921808
   setupds time 3.8299E-01 seconds   0  8    70904907      524386
 
 nElements   max/min/bal: 3973 3972 1.00
 nMessages   max/min/avg: 25 5 12.79
 msgSize     max/min/avg: 23433 1 7321.26
 msgSizeSum  max/min/avg: 120113 63408 88627.91
 
 max multiplicity           44
 done :: setup mesh topology
  
 call usrdat
 done :: usrdat

 generate geometry data
 done :: generate geometry data
  
 call usrdat2
 done :: usrdat2

  3.9629E-15  3.9299E-15  7.1054E-15  7.6927E-16  7.6927E-16  9.8206E-16 xyz repair 1
  3.9629E-15  3.9299E-15  7.1054E-15  6.9253E-15  6.5223E-15  9.3390E-15 xyz repair 2
  3.7848E-15  3.5527E-15  7.1054E-15  5.8127E-15  6.1523E-15  8.9797E-15 xyz repair 3
  3.7863E-15  2.8686E-15  3.8168E-15  3.7863E-15  2.8686E-15  3.8168E-15 xyz repair 4
 regenerate geometry data           1
 done :: regenerate geometry data           1
  
 regenerate geometry data           1
 done :: regenerate geometry data           1
  
 verify mesh topology
  -13.858251128023300        13.858078221547562       Xrange
  -13.858092645782323        13.858190303122345       Yrange
  -14.673319816589355        17.691219329833984       Zrange
 done :: verify mesh topology
  
 mesh metrics:
 GLL grid spacing min/max    : 2.21E-04 3.02E-01
 scaled Jacobian  min/max/avg: 2.59E-02 9.91E-01 3.71E-01
 aspect ratio     min/max/avg: 1.12E+00 1.08E+02 1.26E+01

 call usrdat3
 done :: usrdat3

gridpoints unique/tot:     184172283    268485632
dofs vel/pr:               175531272    184064426
 nek setup done in    9.2444E+00 s

 set initial conditions
 Checking restart options: r5.fld                                                                                                                              
 nekuic (1) for ifld            1
 Reading checkpoint data 
       FILE:/gpfs/alpine/csc262/scratch/malachi/siam-pp-22/pb1568/semfem-fp32-22/r5.fld                                                         

        0  2.0000E+01 done :: Read checkpoint data
                              avg data-throughput =     5.0GB/s
                              io-nodes =   132

 xyz min    -13.858      -13.858      -14.673    
 uvwpt min  -6.4231      -7.8486      -7.9142      -20.893       0.0000    
 PS min      0.0000       0.0000       0.0000      0.99000E+22
 xyz max     13.858       13.858       17.691    
 uvwpt max   6.5953       7.1037       10.614       19.461       0.0000    
 PS max      0.0000       0.0000       0.0000     -0.99000E+22
 Restart: recompute geom. factors.
 regenerate geometry data           1
 done :: regenerate geometry data           1
  
 done :: set initial conditions
  
calling nek_userchk ...
 xyz min    -13.858      -13.858      -14.673    
 xyz max     13.858       13.858       17.691    

loading mesh from nek ... NboundaryIDs: 4, NboundaryFaces: 178458 done (0.00439379s)
generating mesh ... Nq: 8 cubNq: 11
computing geometric factors ... J [1.11222e-05,0.168148] done (0.28447s)
timing oogs modes: 0.000500784s 0.00032203s 0.000335522s 0.000289753s 0.00030523s 0.000274107s used config: 3.0.1
min 47% of the local elements are internal
timing oogs modes: 0.00155842s 0.000694696s 0.000764642s 0.000722065s 0.000512085s 0.000521372s used config: 3.0.0
loading ns kernels ... done (0.000649655s)
copying solution from nek
calling udf_setup ... done
================ ELLIPTIC SETUP VELOCITY ================
bID 1 -> bcType fixedValue
bID 2 -> bcType zeroGradient
bID 3 -> bcType zeroValue
bID 4 -> bcType zeroValue
allNeumann = 0 
loading elliptic kernels ... done (0.000182356s)
timing oogs modes: 0.00157063s 0.000706051s 0.000760354s 0.000675758s 0.000446626s 0.00052552s used config: 3.0.0
timing oogs modes: 0.00154058s 0.000968355s 0.00104463s 0.00106601s 0.000625114s 0.000616839s used config: 3.0.1
building Jacobi preconditioner ... done (0.34067s)
done (4.7609s)
================ ELLIPTIC SETUP PRESSURE ================
allNeumann = 0 
loading elliptic kernels ... done (0.000387388s)
timing oogs modes: 0.000505362s 0.000328237s 0.000324692s 0.000301441s 0.000292764s 0.000270924s used config: 3.0.1
timing oogs modes: 0.000530749s 0.000468124s 0.000480239s 0.000455764s 0.000366389s 0.000348368s used config: 3.0.1
setup SEMFEM preconditioner ... 
building matrix ... done (7.19014s)
AMGX version 2.2.0.132-opensource
Built on Sep 22 2021, 17:25:35
Compiled with CUDA Runtime 11.0, using CUDA driver 11.0
Using CUDA-Aware MPI (GPU Direct) communicator...
AMG Grid:
         Number of Levels: 11
            LVL         ROWS               NNZ    SPRSTY       Mem (GB)
         --------------------------------------------------------------
           0(D)    184064426        3454534578  1.02e-07           31.7
           1(D)     77851179        2612910471  4.31e-07           48.2
           2(D)     29321595        1488457111  1.73e-06           30.5
           3(D)      9845483         750031009  7.74e-06           17.7
           4(D)      2425158         240043832  4.08e-05           7.12
           5(D)       362788          40017918  0.000304           1.79
           6(D)        37631           3392217    0.0024          0.295
           7(D)         4137            348745    0.0204         0.0811
           8(D)          463             32089      0.15         0.0204
           9(D)           62              2584     0.672        0.00157
          10(D)           10               100         1       2.08e-05
         --------------------------------------------------------------
         Grid Complexity: 1.65112
         Operator Complexity: 2.48652
         Total Memory Usage: 137.42 GB
         --------------------------------------------------------------
done (45.2269s)
done (45.227s)
done (51.1302s)
copying solution to nek

settings:

key: FORMAT,                                                  value: 1.0
key: CONSTANT FLOW RATE,                                      value: FALSE
key: ELEMENT TYPE,                                            value: 12
key: ELEMENT MAP,                                             value: ISOPARAMETRIC
key: MESH DIMENSION,                                          value: 3
key: NUMBER OF SCALARS,                                       value: 0
key: SCALAR MAXIMUM ITERATIONS,                               value: 200
key: TIME INTEGRATOR,                                         value: TOMBO2
key: MESH INTEGRATION ORDER,                                  value: 3
key: SUBCYCLING STEPS,                                        value: 2
key: SUBCYCLING TIME ORDER,                                   value: 4
key: SUBCYCLING TIME STAGE NUMBER,                            value: 4
key: CASENAME,                                                value: peb1568_n2t2bnb
key: UDF OKL FILE,                                            value: peb1568_n2t2bnb.oudf
key: UDF FILE,                                                value: peb1568_n2t2bnb.udf
key: NEK USR FILE,                                            value: peb1568_n2t2bnb.usr
key: MESH FILE,                                               value: peb1568_n2t2bnb.re2
key: DEVICE NUMBER,                                           value: 0
key: PLATFORM NUMBER,                                         value: 0
key: VERBOSE,                                                 value: FALSE
key: ADVECTION,                                               value: TRUE
key: ADVECTION TYPE,                                          value: CUBATURE+CONVECTIVE
key: RESTART FROM FILE,                                       value: 1
key: SOLUTION OUTPUT INTERVAL,                                value: 0.000000
key: SOLUTION OUTPUT CONTROL,                                 value: STEPS
key: REGULARIZATION METHOD,                                   value: RELAXATION
key: START TIME,                                              value: 2.000000e+01
key: VELOCITY MAXIMUM ITERATIONS,                             value: 200
key: VELOCITY BLOCK SOLVER,                                   value: TRUE
key: VELOCITY KRYLOV SOLVER,                                  value: PCG
key: VELOCITY BASIS,                                          value: NODAL
key: VELOCITY PRECONDITIONER,                                 value: JACOBI
key: VELOCITY DISCRETIZATION,                                 value: CONTINUOUS
key: STRESSFORMULATION,                                       value: FALSE
key: ELLIPTIC INTEGRATION,                                    value: NODAL
key: PRESSURE MAXIMUM ITERATIONS,                             value: 200
key: GALERKIN COARSE MATRIX,                                  value: FALSE
key: PRESSURE KRYLOV SOLVER,                                  value: PGMRES+FLEXIBLE
key: PRESSURE PRECONDITIONER,                                 value: SEMFEM
key: PRESSURE DISCRETIZATION,                                 value: CONTINUOUS
key: PRESSURE BASIS,                                          value: NODAL
key: AMG SOLVER,                                              value: BOOMERAMG
key: AMG SOLVER PRECISION,                                    value: FP64
key: AMG SOLVER LOCATION,                                     value: CPU
key: PRESSURE PARALMOND CYCLE,                                value: VCYCLE
key: PRESSURE MULTIGRID COARSE SOLVE,                         value: TRUE
key: PRESSURE MULTIGRID COARSE SEMFEM,                        value: FALSE
key: PRESSURE MULTIGRID SMOOTHER,                             value: CHEBYSHEV+ASM
key: PRESSURE MULTIGRID DOWNWARD SMOOTHER,                    value: ASM
key: PRESSURE MULTIGRID UPWARD SMOOTHER,                      value: ASM
key: PRESSURE MULTIGRID CHEBYSHEV DEGREE,                     value: 2
key: PRESSURE MULTIGRID CHEBYSHEV MIN EIGENVALUE BOUND FACTOR,value: 0.1
key: PRESSURE MULTIGRID CHEBYSHEV MAX EIGENVALUE BOUND FACTOR,value: 1.1
key: PRESSURE INITIAL GUESS,                                  value: PROJECTION-ACONJ
key: PRESSURE RESIDUAL PROJECTION VECTORS,                    value: 10
key: PRESSURE RESIDUAL PROJECTION START,                      value: 5
key: PARALMOND SMOOTH COARSEST,                               value: FALSE
key: ENABLE FLOATCOMMHALF GS SUPPORT,                         value: FALSE
key: MOVING MESH,                                             value: FALSE
key: ENABLE OVERLAP,                                          value: TRUE
key: VARIABLE DT,                                             value: FALSE
key: THREAD MODEL,                                            value: CUDA
key: RESTART FILE NAME,                                       value: r5.fld
key: POLYNOMIAL DEGREE,                                       value: 7
key: DT,                                                      value: 5.000000e-04
key: NUMBER TIMESTEPS,                                        value: 200
key: CUBATURE POLYNOMIAL DEGREE,                              value: 10
key: HPFRT STRENGTH,                                          value: 2.000000e+02
key: HPFRT MODES,                                             value: 2.000000e+00
key: VELOCITY REGULARIZATION METHOD,                          value: RELAXATION
key: VELOCITY HPFRT MODES,                                    value: 2.000000e+00
key: VELOCITY HPFRT STRENGTH,                                 value: 2.000000e+02
key: PRESSURE SOLVER TOLERANCE,                               value: 1.000000e-04
key: PRESSURE SEMFEM SOLVER,                                  value: AMGX
key: PRESSURE SEMFEM SOLVER PRECISION,                        value: FP32
key: AMGX CONFIG FILE,                                        value: amgx.json
key: VELOCITY SOLVER TOLERANCE,                               value: 1.000000e-06
key: DENSITY,                                                 value: 1.000000e+00
key: VISCOSITY,                                               value: 1.000000e-04
key: BUILD ONLY,                                              value: FALSE
key: DATA FILE,                                               value: /gpfs/alpine/csc262/scratch/malachi/siam-pp-22/pb1568/semfem-fp32-22/.cache/udf/udf.okl
key: CI-MODE,                                                 value: 0
key: CHECKPOINT OUTPUT MESH,                                  value: FALSE

occa memory usage: 2.90245 GB
initialization took 150.805 s

timestepping for 200 steps ...
  P  : iter 030  resNorm00 1.21e+00  resNorm0 1.21e+00  resNorm 6.99e-05
  UVW: iter 003  resNorm00 1.17e-01  resNorm0 1.17e-01  resNorm 4.27e-07  divErrNorms 9.72e-07 4.95e-01
step= 1  t= 2.00005000e+01  dt=5.0e-04  C= 2.05  UVW: 3  P: 30  eTimeStep= 4.11e+00s eTime= 4.11221e+00s
  P  : iter 037  resNorm00 1.50e+00  resNorm0 1.50e+00  resNorm 9.81e-05
  UVW: iter 003  resNorm00 1.76e-01  resNorm0 1.76e-01  resNorm 2.10e-07  divErrNorms 9.16e-07 4.93e-01
step= 2  t= 2.00010000e+01  dt=5.0e-04  C= 1.98  UVW: 3  P: 37  eTimeStep= 7.51e-01s eTime= 4.86299e+00s
  P  : iter 051  resNorm00 3.99e-01  resNorm0 3.99e-01  resNorm 8.30e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.09e-07  divErrNorms 8.67e-07 4.90e-01
step= 3  t= 2.00015000e+01  dt=5.0e-04  C= 1.93  UVW: 3  P: 51  eTimeStep= 1.00e+00s eTime= 5.86736e+00s
  P  : iter 040  resNorm00 2.18e-01  resNorm0 2.18e-01  resNorm 9.31e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.08e-07  divErrNorms 8.22e-07 4.87e-01
step= 4  t= 2.00020000e+01  dt=5.0e-04  C= 1.97  UVW: 3  P: 40  eTimeStep= 7.97e-01s eTime= 6.66387e+00s
  P  : iter 027  resNorm00 1.71e-01  resNorm0 1.71e-01  resNorm 5.88e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.07e-07  divErrNorms 7.88e-07 4.85e-01
step= 5  t= 2.00025000e+01  dt=5.0e-04  C= 1.98  UVW: 3  P: 27  eTimeStep= 5.53e-01s eTime= 7.21707e+00s
  P  : iter 013  resNorm00 1.54e-01  resNorm0 3.45e-02  resNorm 9.64e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.07e-07  divErrNorms 7.58e-07 4.83e-01
step= 6  t= 2.00030000e+01  dt=5.0e-04  C= 1.98  UVW: 3  P: 13  eTimeStep= 2.93e-01s eTime= 7.51014e+00s
  P  : iter 012  resNorm00 1.47e-01  resNorm0 1.42e-02  resNorm 9.65e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.06e-07  divErrNorms 7.34e-07 4.82e-01
step= 7  t= 2.00035000e+01  dt=5.0e-04  C= 1.97  UVW: 3  P: 12  eTimeStep= 2.73e-01s eTime= 7.78357e+00s
  P  : iter 010  resNorm00 1.43e-01  resNorm0 6.62e-03  resNorm 7.48e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.06e-07  divErrNorms 7.15e-07 4.81e-01
step= 8  t= 2.00040000e+01  dt=5.0e-04  C= 1.94  UVW: 3  P: 10  eTimeStep= 2.35e-01s eTime= 8.01876e+00s
  P  : iter 010  resNorm00 1.41e-01  resNorm0 4.48e-03  resNorm 9.07e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.06e-07  divErrNorms 6.95e-07 4.80e-01
step= 9  t= 2.00045000e+01  dt=5.0e-04  C= 1.89  UVW: 3  P: 10  eTimeStep= 2.38e-01s eTime= 8.25636e+00s
  P  : iter 009  resNorm00 1.40e-01  resNorm0 4.56e-03  resNorm 8.46e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 6.76e-07 4.79e-01
step= 10  t= 2.00050000e+01  dt=5.0e-04  C= 1.83  UVW: 3  P: 9  eTimeStep= 2.16e-01s eTime= 8.47263e+00s
  P  : iter 008  resNorm00 1.39e-01  resNorm0 2.48e-03  resNorm 7.54e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 6.64e-07 4.78e-01
step= 11  t= 2.00055000e+01  dt=5.0e-04  C= 1.76  UVW: 3  P: 8  eTimeStep= 1.96e-01s eTime= 8.66813e+00s
  P  : iter 008  resNorm00 1.38e-01  resNorm0 2.86e-03  resNorm 9.63e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 6.52e-07 4.77e-01
step= 12  t= 2.00060000e+01  dt=5.0e-04  C= 1.69  UVW: 3  P: 8  eTimeStep= 1.95e-01s eTime= 8.86287e+00s
  P  : iter 006  resNorm00 1.38e-01  resNorm0 8.19e-04  resNorm 8.25e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 6.33e-07 4.77e-01
step= 13  t= 2.00065000e+01  dt=5.0e-04  C= 1.74  UVW: 3  P: 6  eTimeStep= 1.58e-01s eTime= 9.02131e+00s
  P  : iter 008  resNorm00 1.37e-01  resNorm0 2.14e-03  resNorm 8.52e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 6.17e-07 4.76e-01
step= 14  t= 2.00070000e+01  dt=5.0e-04  C= 1.79  UVW: 3  P: 8  eTimeStep= 1.95e-01s eTime= 9.21648e+00s
  P  : iter 008  resNorm00 1.37e-01  resNorm0 1.06e-03  resNorm 9.28e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 6.09e-07 4.76e-01
step= 15  t= 2.00075000e+01  dt=5.0e-04  C= 1.82  UVW: 3  P: 8  eTimeStep= 2.01e-01s eTime= 9.41791e+00s
  P  : iter 010  resNorm00 1.37e-01  resNorm0 8.00e-03  resNorm 9.94e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 6.02e-07 4.76e-01
step= 16  t= 2.00080000e+01  dt=5.0e-04  C= 1.86  UVW: 3  P: 10  eTimeStep= 2.41e-01s eTime= 9.65902e+00s
  P  : iter 008  resNorm00 1.37e-01  resNorm0 1.02e-03  resNorm 6.95e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 5.86e-07 4.75e-01
step= 17  t= 2.00085000e+01  dt=5.0e-04  C= 1.91  UVW: 3  P: 8  eTimeStep= 2.08e-01s eTime= 9.86679e+00s
  P  : iter 008  resNorm00 1.37e-01  resNorm0 1.53e-03  resNorm 9.61e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 5.72e-07 4.75e-01
step= 18  t= 2.00090000e+01  dt=5.0e-04  C= 1.95  UVW: 3  P: 8  eTimeStep= 1.98e-01s eTime= 1.00651e+01s
  P  : iter 009  resNorm00 1.37e-01  resNorm0 1.27e-03  resNorm 9.45e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 5.67e-07 4.75e-01
step= 19  t= 2.00095000e+01  dt=5.0e-04  C= 1.99  UVW: 3  P: 9  eTimeStep= 2.17e-01s eTime= 1.02819e+01s
  P  : iter 008  resNorm00 1.37e-01  resNorm0 1.66e-03  resNorm 9.31e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 5.59e-07 4.75e-01
step= 20  t= 2.00100000e+01  dt=5.0e-04  C= 2.08  UVW: 3  P: 8  eTimeStep= 1.98e-01s eTime= 1.04800e+01s
  P  : iter 008  resNorm00 1.37e-01  resNorm0 1.35e-03  resNorm 9.14e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 5.47e-07 4.75e-01
step= 21  t= 2.00105000e+01  dt=5.0e-04  C= 2.24  UVW: 3  P: 8  eTimeStep= 1.98e-01s eTime= 1.06783e+01s
  P  : iter 008  resNorm00 1.37e-01  resNorm0 9.46e-04  resNorm 8.57e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 5.37e-07 4.75e-01
step= 22  t= 2.00110000e+01  dt=5.0e-04  C= 2.34  UVW: 3  P: 8  eTimeStep= 1.98e-01s eTime= 1.08761e+01s
  P  : iter 009  resNorm00 1.37e-01  resNorm0 9.23e-04  resNorm 8.60e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 5.30e-07 4.74e-01
step= 23  t= 2.00115000e+01  dt=5.0e-04  C= 2.39  UVW: 3  P: 9  eTimeStep= 2.17e-01s eTime= 1.10936e+01s
  P  : iter 009  resNorm00 1.37e-01  resNorm0 7.75e-04  resNorm 8.26e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 5.20e-07 4.74e-01
step= 24  t= 2.00120000e+01  dt=5.0e-04  C= 2.37  UVW: 3  P: 9  eTimeStep= 2.18e-01s eTime= 1.13113e+01s
  P  : iter 010  resNorm00 1.37e-01  resNorm0 1.17e-03  resNorm 9.43e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 5.11e-07 4.74e-01
step= 25  t= 2.00125000e+01  dt=5.0e-04  C= 2.30  UVW: 3  P: 10  eTimeStep= 2.37e-01s eTime= 1.15478e+01s
  P  : iter 011  resNorm00 1.37e-01  resNorm0 8.01e-03  resNorm 7.57e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 5.03e-07 4.74e-01
step= 26  t= 2.00130000e+01  dt=5.0e-04  C= 2.17  UVW: 3  P: 11  eTimeStep= 2.54e-01s eTime= 1.18016e+01s
  P  : iter 008  resNorm00 1.36e-01  resNorm0 9.62e-04  resNorm 7.92e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 4.94e-07 4.74e-01
step= 27  t= 2.00135000e+01  dt=5.0e-04  C= 2.02  UVW: 3  P: 8  eTimeStep= 1.97e-01s eTime= 1.19991e+01s
  P  : iter 009  resNorm00 1.36e-01  resNorm0 1.04e-03  resNorm 9.20e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 4.85e-07 4.74e-01
step= 28  t= 2.00140000e+01  dt=5.0e-04  C= 2.00  UVW: 3  P: 9  eTimeStep= 2.18e-01s eTime= 1.22168e+01s
  P  : iter 009  resNorm00 1.36e-01  resNorm0 7.97e-04  resNorm 9.83e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 4.77e-07 4.74e-01
step= 29  t= 2.00145000e+01  dt=5.0e-04  C= 1.98  UVW: 3  P: 9  eTimeStep= 2.17e-01s eTime= 1.24343e+01s
  P  : iter 010  resNorm00 1.36e-01  resNorm0 9.94e-04  resNorm 9.22e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 4.68e-07 4.74e-01
step= 30  t= 2.00150000e+01  dt=5.0e-04  C= 1.96  UVW: 3  P: 10  eTimeStep= 2.35e-01s eTime= 1.26695e+01s
  P  : iter 011  resNorm00 1.36e-01  resNorm0 1.18e-03  resNorm 7.83e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 4.60e-07 4.74e-01
step= 31  t= 2.00155000e+01  dt=5.0e-04  C= 1.93  UVW: 3  P: 11  eTimeStep= 2.54e-01s eTime= 1.29237e+01s
  P  : iter 010  resNorm00 1.36e-01  resNorm0 9.12e-04  resNorm 8.68e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 4.52e-07 4.74e-01
step= 32  t= 2.00160000e+01  dt=5.0e-04  C= 1.90  UVW: 3  P: 10  eTimeStep= 2.36e-01s eTime= 1.31593e+01s
  P  : iter 008  resNorm00 1.36e-01  resNorm0 8.41e-04  resNorm 7.91e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 4.44e-07 4.74e-01
step= 33  t= 2.00165000e+01  dt=5.0e-04  C= 1.87  UVW: 3  P: 8  eTimeStep= 1.97e-01s eTime= 1.33568e+01s
  P  : iter 008  resNorm00 1.36e-01  resNorm0 8.82e-04  resNorm 7.61e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 4.35e-07 4.74e-01
step= 34  t= 2.00170000e+01  dt=5.0e-04  C= 1.86  UVW: 3  P: 8  eTimeStep= 1.98e-01s eTime= 1.35551e+01s
  P  : iter 008  resNorm00 1.36e-01  resNorm0 8.88e-04  resNorm 8.78e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 4.26e-07 4.74e-01
step= 35  t= 2.00175000e+01  dt=5.0e-04  C= 2.02  UVW: 3  P: 8  eTimeStep= 1.99e-01s eTime= 1.37541e+01s
  P  : iter 012  resNorm00 1.36e-01  resNorm0 8.02e-03  resNorm 9.43e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 4.18e-07 4.74e-01
step= 36  t= 2.00180000e+01  dt=5.0e-04  C= 2.22  UVW: 3  P: 12  eTimeStep= 2.73e-01s eTime= 1.40271e+01s
  P  : iter 008  resNorm00 1.36e-01  resNorm0 1.02e-03  resNorm 9.71e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 4.11e-07 4.74e-01
step= 37  t= 2.00185000e+01  dt=5.0e-04  C= 2.34  UVW: 3  P: 8  eTimeStep= 1.98e-01s eTime= 1.42251e+01s
  P  : iter 010  resNorm00 1.36e-01  resNorm0 1.00e-03  resNorm 9.97e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 4.02e-07 4.74e-01
step= 38  t= 2.00190000e+01  dt=5.0e-04  C= 2.46  UVW: 3  P: 10  eTimeStep= 2.35e-01s eTime= 1.44605e+01s
  P  : iter 011  resNorm00 1.36e-01  resNorm0 1.71e-03  resNorm 8.18e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 3.93e-07 4.74e-01
step= 39  t= 2.00195000e+01  dt=5.0e-04  C= 2.58  UVW: 3  P: 11  eTimeStep= 2.56e-01s eTime= 1.47169e+01s
  P  : iter 010  resNorm00 1.36e-01  resNorm0 1.06e-03  resNorm 9.69e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 3.86e-07 4.74e-01
step= 40  t= 2.00200000e+01  dt=5.0e-04  C= 2.64  UVW: 3  P: 10  eTimeStep= 2.36e-01s eTime= 1.49530e+01s
  P  : iter 011  resNorm00 1.37e-01  resNorm0 1.04e-03  resNorm 9.45e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.05e-07  divErrNorms 3.78e-07 4.74e-01
step= 41  t= 2.00205000e+01  dt=5.0e-04  C= 2.62  UVW: 3  P: 11  eTimeStep= 2.54e-01s eTime= 1.52072e+01s
  P  : iter 019  resNorm00 1.37e-01  resNorm0 1.39e-03  resNorm 9.99e-05
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.06e-07  divErrNorms 3.69e-07 4.74e-01
step= 42  t= 2.00210000e+01  dt=5.0e-04  C= 2.55  UVW: 3  P: 19  eTimeStep= 4.04e-01s eTime= 1.56115e+01s
  P  : iter 200  resNorm00 1.37e-01  resNorm0 1.08e-03  resNorm 3.04e-04
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.06e-07  divErrNorms 3.60e-07 4.74e-01
step= 43  t= 2.00215000e+01  dt=5.0e-04  C= 2.47  UVW: 3  P: 200  eTimeStep= 3.82e+00s eTime= 1.94336e+01s
  P  : iter 200  resNorm00 1.37e-01  resNorm0 1.52e-03  resNorm 1.51e-03
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.06e-07  divErrNorms 3.54e-07 4.74e-01
step= 44  t= 2.00220000e+01  dt=5.0e-04  C= 2.47  UVW: 3  P: 200  eTimeStep= 3.82e+00s eTime= 2.32538e+01s
  P  : iter 200  resNorm00 1.37e-01  resNorm0 4.62e-03  resNorm 4.60e-03
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.06e-07  divErrNorms 3.51e-07 4.74e-01
step= 45  t= 2.00225000e+01  dt=5.0e-04  C= 2.44  UVW: 3  P: 200  eTimeStep= 3.79e+00s eTime= 2.70469e+01s
  P  : iter 200  resNorm00 1.37e-01  resNorm0 1.36e-02  resNorm 1.36e-02
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.06e-07  divErrNorms 3.53e-07 4.74e-01
step= 46  t= 2.00230000e+01  dt=5.0e-04  C= 2.36  UVW: 3  P: 200  eTimeStep= 3.80e+00s eTime= 3.08484e+01s
  P  : iter 200  resNorm00 1.40e-01  resNorm0 3.63e-02  resNorm 3.63e-02
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.06e-07  divErrNorms 3.60e-07 4.74e-01
step= 47  t= 2.00235000e+01  dt=5.0e-04  C= 2.26  UVW: 3  P: 200  eTimeStep= 3.79e+00s eTime= 3.46396e+01s
  P  : iter 200  resNorm00 1.54e-01  resNorm0 7.89e-02  resNorm 7.89e-02
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.06e-07  divErrNorms 3.70e-07 4.74e-01
step= 48  t= 2.00240000e+01  dt=5.0e-04  C= 2.14  UVW: 3  P: 200  eTimeStep= 3.80e+00s eTime= 3.84392e+01s
  P  : iter 200  resNorm00 1.96e-01  resNorm0 1.48e-01  resNorm 1.48e-01
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.06e-07  divErrNorms 3.83e-07 4.74e-01
step= 49  t= 2.00245000e+01  dt=5.0e-04  C= 2.03  UVW: 3  P: 200  eTimeStep= 3.79e+00s eTime= 4.22294e+01s
  P  : iter 200  resNorm00 2.78e-01  resNorm0 2.48e-01  resNorm 2.48e-01
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.06e-07  divErrNorms 3.99e-07 4.74e-01
step= 50  t= 2.00250000e+01  dt=5.0e-04  C= 1.98  UVW: 3  P: 200  eTimeStep= 3.80e+00s eTime= 4.60339e+01s
  P  : iter 200  resNorm00 4.05e-01  resNorm0 3.86e-01  resNorm 3.86e-01
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.06e-07  divErrNorms 4.19e-07 4.74e-01
step= 51  t= 2.00255000e+01  dt=5.0e-04  C= 1.99  UVW: 3  P: 200  eTimeStep= 3.80e+00s eTime= 4.98349e+01s
  P  : iter 200  resNorm00 5.79e-01  resNorm0 5.68e-01  resNorm 5.68e-01
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.06e-07  divErrNorms 4.47e-07 4.75e-01
step= 52  t= 2.00260000e+01  dt=5.0e-04  C= 2.00  UVW: 3  P: 200  eTimeStep= 3.80e+00s eTime= 5.36339e+01s
  P  : iter 200  resNorm00 8.04e-01  resNorm0 7.97e-01  resNorm 7.97e-01
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.07e-07  divErrNorms 4.79e-07 4.77e-01
step= 53  t= 2.00265000e+01  dt=5.0e-04  C= 2.00  UVW: 3  P: 200  eTimeStep= 3.80e+00s eTime= 5.74324e+01s
  P  : iter 200  resNorm00 1.08e+00  resNorm0 1.08e+00  resNorm 1.08e+00
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.07e-07  divErrNorms 5.13e-07 4.80e-01
step= 54  t= 2.00270000e+01  dt=5.0e-04  C= 1.99  UVW: 3  P: 200  eTimeStep= 3.80e+00s eTime= 6.12356e+01s
  P  : iter 200  resNorm00 1.42e+00  resNorm0 1.42e+00  resNorm 1.42e+00
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.07e-07  divErrNorms 5.52e-07 4.85e-01
step= 55  t= 2.00275000e+01  dt=5.0e-04  C= 2.02  UVW: 3  P: 200  eTimeStep= 3.80e+00s eTime= 6.50342e+01s
  P  : iter 200  resNorm00 1.81e+00  resNorm0 1.81e+00  resNorm 1.81e+00
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.07e-07  divErrNorms 5.92e-07 4.92e-01
step= 56  t= 2.00280000e+01  dt=5.0e-04  C= 2.24  UVW: 3  P: 200  eTimeStep= 3.81e+00s eTime= 6.88449e+01s
  P  : iter 200  resNorm00 2.28e+00  resNorm0 2.28e+00  resNorm 2.28e+00
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.07e-07  divErrNorms 6.35e-07 5.01e-01
step= 57  t= 2.00285000e+01  dt=5.0e-04  C= 2.51  UVW: 3  P: 200  eTimeStep= 3.79e+00s eTime= 7.26324e+01s
  P  : iter 200  resNorm00 2.80e+00  resNorm0 2.80e+00  resNorm 2.80e+00
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.07e-07  divErrNorms 6.84e-07 5.15e-01
step= 58  t= 2.00290000e+01  dt=5.0e-04  C= 2.82  UVW: 3  P: 200  eTimeStep= 3.79e+00s eTime= 7.64270e+01s
  P  : iter 200  resNorm00 3.40e+00  resNorm0 3.40e+00  resNorm 3.40e+00
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.07e-07  divErrNorms 7.33e-07 5.32e-01
step= 59  t= 2.00295000e+01  dt=5.0e-04  C= 3.15  UVW: 3  P: 200  eTimeStep= 3.79e+00s eTime= 8.02188e+01s
  P  : iter 200  resNorm00 4.07e+00  resNorm0 4.07e+00  resNorm 4.07e+00
  UVW: iter 003  resNorm00 1.75e-01  resNorm0 1.75e-01  resNorm 2.07e-07  divErrNorms 7.93e-07 5.54e-01
step= 60  t= 2.00300000e+01  dt=5.0e-04  C= 3.48  UVW: 3  P: 200  eTimeStep= 3.81e+00s eTime= 8.40251e+01s
Unreasonable res0Norm!
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 54 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 66 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 55 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 67 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 36 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 56 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 37 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 57 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 68 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 42 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 90 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 58 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 43 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 91 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 38 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 69 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 1 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 44 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 18 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 92 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 59 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 45 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 93 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 70 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 39 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 46 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 19 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 94 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 4 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 47 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 95 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 71 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 6 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 20 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 30 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 21 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 23 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 102 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 40 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 22 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 3 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 31 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 96 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 114 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 103 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 33 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 115 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 7 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 116 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 41 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 2 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 104 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 48 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 118 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 35 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 119 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 8 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 117 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 105 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 9 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 49 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 24 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 106 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 97 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 32 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 5 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 10 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 107 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 34 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 50 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 11 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 78 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 98 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 12 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 126 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 51 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 72 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 99 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 79 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 13 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 127 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 25 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 80 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 52 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 120 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 128 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 82 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 101 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 14 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 131 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 81 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 53 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 100 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 15 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 130 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 83 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 129 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 16 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 26 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 121 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 17 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 122 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 27 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 73 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 28 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 74 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 123 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 29 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 125 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 76 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 124 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 75 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 77 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------

------------------------------------------------------------
Sender: LSF System <lsfadmin@batch3>
Subject: Job 1453925: <nekRS_peb1568_n2t2bnb> in cluster <summit> Exited

Job <nekRS_peb1568_n2t2bnb> was submitted from host <login1> by user <malachi> in cluster <summit> at Sun Sep 26 12:08:56 2021
Job was executed on host(s) <1*batch3>, in queue <batch>, as user <malachi> in cluster <summit> at Sun Sep 26 12:12:12 2021
                            <42*a15n09>
                            <42*a15n10>
                            <42*a15n11>
                            <42*a15n13>
                            <42*a15n14>
                            <42*a15n15>
                            <42*a15n16>
                            <42*a15n17>
                            <42*a15n18>
                            <42*a16n01>
                            <42*a16n02>
                            <42*a16n03>
                            <42*a16n05>
                            <42*a16n06>
                            <42*a16n07>
                            <42*a16n08>
                            <42*a16n10>
                            <42*a16n11>
                            <42*a16n12>
                            <42*b04n10>
                            <42*b04n15>
                            <42*b04n16>
</ccs/home/malachi> was used as the home directory.
</gpfs/alpine/scratch/malachi/csc262/siam-pp-22/pb1568/semfem-fp32-22> was used as the working directory.
Started at Sun Sep 26 12:12:12 2021
Terminated at Sun Sep 26 12:16:46 2021
Results reported at Sun Sep 26 12:16:46 2021

The output (if any) is above this job summary.

